{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d81f71",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw02.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e1d476",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "#### Homework 2 Supplemental Notebook\n",
    "    \n",
    "# Empirical Risk and Simple Linear Regression\n",
    "\n",
    "### EECS 245, Fall 2025 at the University of Michigan\n",
    "    \n",
    "</div>\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Most homeworks will have Jupyter Notebooks, like this one, designed to supplement the theoretical problems. \n",
    "\n",
    "To write and run code in this notebook, you have two options:\n",
    "\n",
    "1. **Use the EECS 245 DataHub.** To do this, click the link provided in the Homework 2 PDF. Before doing so, read the instructions on the [**Tech Support**](https://eecs245.org/tech-support/#option-1-using-the-eecs-245-datahub) page on how to use the DataHub.\n",
    "1. **Set up a Jupyter Notebook environment locally, and use `git` to clone our course repository.** For instructions on how to do this, see the [**Tech Support**](https://eecs245.org/tech-support) page of the course website.\n",
    "\n",
    "To receive credit for the programming portion of the homework, you'll need to submit your completed notebook to the autograder on Gradescope. Your submission time for Homework 2 is the **latter** of your PDF and code submission times.\n",
    "\n",
    "Remember that homework problems have hidden test cases. The public test cases in your notebook only verify that your answer is in the correct format and on the right track; your results on the hidden tests will be available to you on Gradescope after we release grades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c257bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3bd812",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Problem 5: Simple LADs 🧍\n",
    "\n",
    "---\n",
    "\n",
    "In Chapter 1.4, we explored simple linear regression, and defined it as the problem of finding the values of $w_0$ (intercept) and $w_1$ (slope) that minimize mean squared error for the model $h(x_i) = w_0 + w_1 x_i$.\n",
    "\n",
    "\\begin{align*}\n",
    "R_{\\text{sq}}(w_0, w_1) &= \\frac{1}{n} \\sum_{i=1}^{n} (y_i -(w_0 + w_1x_i))^2\n",
    "\\end{align*}\n",
    "\n",
    "The optimal slope and intercept were denoted $w_1^*$ and $w_0^*$, respectively, and have closed-form solutions that are stated in Chapter 1.4. When using squared loss to find our optimal parameters, linear regression is often called \"least squares regression.\" \n",
    "\n",
    "**What if we used a different loss function instead?**\n",
    "\n",
    "In this question, we'll implement another type of linear regression: simple least absolute deviation (LAD) regression. LAD regression uses absolute loss to measure the quality of predictions, rather than squared loss. Put another way, to find the optimal slope $w_1^*$ and intercept $w_0^*$ for LAD regression, we minimize mean absolute error:\n",
    "\n",
    "\\begin{align*}\n",
    "R_{\\text{abs}}(w_0, w_1) &= \\frac{1}{n} \\sum_{i=1}^{n} |y_i -(w_0 + w_1x_i)|\n",
    "\\end{align*}\n",
    "\n",
    "The \"simple\" in \"simple LAD\" refers to the fact that our hypothesis function $h(x_i) = w_0 + w_1 x_i$, like in regular simple linear regression, only uses a single input feature.\n",
    "\n",
    "Since absolute value functions are not differentiable, we cannot just take partial derivatives of $R_{\\text{abs}}$ with respect to $w_0$ and $w_1$, set them equal to zero, and solve for the values of $w_0$ and $w_1$, as we did to minimize $R_{\\text{sq}}$. This was more tractable in the constant model case, but in general it'll require techniques beyond the scope of this class.\n",
    "\n",
    "In order to generate the optimal LAD regression line we are going to leverage the following theorem (which, luckily, we won't need to prove):\n",
    "\n",
    "> The regression model that minimizes mean absolute error passes directly through at least $k$ points, where $k$ is the number of parameters of the model.\n",
    "\n",
    "This theorem is useful to us because it allows us to adopt a very conceptually simple, albeit not very efficient, strategy to compute an optimal simple LAD regression line. Since our hypothesis function has $k = 2$ parameters, an intercept $w_0$ and a slope $w_1$, we can simply:\n",
    "\n",
    "1. Generate all possible pairs of 2 points. We know that the optimal LAD line will pass through at least one of these pairs.\n",
    "1. For each pair of points:\n",
    "    1. Find the equation of the line that passes through the pair. Denote the intercept and slope of this line $w_0$ and $w_1$, respectively.\n",
    "    1. Compute the mean absolute error of the line with intercept $w_0$ and slope $w_1$, i.e. compute $R_\\text{abs}(w_0, w_1)$.\n",
    "1. Return the $(w_0, w_1)$ combination with the minimum value of $R_\\text{abs}(w_0, w_1)$. By the above theorem, this line is guaranteed to minimize mean absolute error.\n",
    "\n",
    "Notice that unlike with simple linear regression, the optimal simple LAD regression line may not be unique!\n",
    "\n",
    "In this question, you will ultimately complete the implementation of the `SimpleLAD` **class**, which can be used as follows:\n",
    "\n",
    "```python\n",
    ">>> model = SimpleLAD()\n",
    ">>> model.fit([1, 2, -1, 4], [15, 6, 7, 8])\n",
    ">>> model.intercept_\n",
    "7.2\n",
    ">>> model.coef_\n",
    "0.2\n",
    ">>> model.predict(5)\n",
    "8.2\n",
    ">>> model.predict([5, -3.5, 5])\n",
    "array([8.2, 6.5, 8.2])\n",
    "```\n",
    "\n",
    "You might recognize this syntax from Lab 2, as this is how model classes work in `sklearn`. Indeed, part of the point of this problem is to get you familarized with how to use machine learning models in code (in addition to understanding how they might be implemented).\n",
    "\n",
    "To help you, we've defined a helper function, `generate_pairs`. Observe how it works below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d78ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(x, y):\n",
    "    from itertools import combinations \n",
    "    tuples = zip(x, y)\n",
    "    return list(combinations(tuples, 2))\n",
    "\n",
    "generate_pairs([1, 2, -1, 4], [15, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dcac1c",
   "metadata": {},
   "source": [
    "Now, it's your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a876d3",
   "metadata": {},
   "source": [
    "### Problem 5a) (2 pts)\n",
    "\n",
    "Complete the implementation of the function `generate_lines`. \n",
    "- `generate_lines` takes in a list, `pairs`, in which each element is a tuple. Each tuple is itself made up of two tuples, corresponding to a pair of points. The input to `generate_lines` may look like:\n",
    "\n",
    "```python\n",
    "    [((1, 2), (3, 7)), ((1, 10), (-4, 20))]\n",
    "```\n",
    "\n",
    "- `generate_lines` returns a list with the same length as `pairs`, in which each element is a tuple of the form `(intercept, slope)`. Element `i` of the returned list should be a tuple containing the intercept and slope of the line passing through the two points in `pairs[i]` (the order of the outputted lines should be the same as the order of the inputted pairs).\n",
    "\n",
    "Example behavior is given below.\n",
    "\n",
    "```python\n",
    ">>> generate_lines([((1, 2), (3, 7)), ((1, 10), (-4, 20))])\n",
    "[(-0.5, 2.5), (12.0, -2.0)]\n",
    "```\n",
    "\n",
    "For more context on the example above:\n",
    "- The input to `generate_lines` contains two pairs of points.\n",
    "- The first pair of points, $(1, 2) \\text{ and } (3, 7)$, sit on the line $y = -0.5 + 2.5x$. The intercept of this line is -0.5 and the slope is 2.5, so the first returned tuple is `(-0.5, 2.5)`.\n",
    "- The second pair of points, $(1, 10) \\text{ and } (-4, 20)$, sit on the line $y = 12 - 2x$. The intercept of this line is 12 and the slope is -2, so the second returned tuple is `(12.0, -2.0)`.\n",
    "\n",
    "\n",
    "Some guidance:\n",
    "- A fact from high school algebra is that given any two points, there is exactly one line that passes through them. You'll need to figure out how to programmatically find the intercept and slope of this line, given any two arbitrary points $(x_1, y_1)$ and $(x_2, y_2)$.\n",
    "- There is theoretically the risk of a `DivisionByZero` error, if a pair of points contains two values with the same $x$-coordinate. We won't test your code on such examples.\n",
    "- You don't have to manually convert the values in the output tuples to floats – this will likely happen automatically because your calculations will involve division, and if it doesn't, don't worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30dcd08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_lines(pairs):\n",
    "    ...\n",
    "    \n",
    "# Feel free to change this input to make sure your function works correctly.\n",
    "generate_lines([((1, 2), (3, 7)), ((1, 10), (-4, 20))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f81348",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"p05_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a2cf50",
   "metadata": {},
   "source": [
    "### Problem 5b) (2 pts)\n",
    "\n",
    "Complete the implementation of the function `mae_of_candidate_line`, which takes in four inputs:\n",
    "- `intercept`, a float,\n",
    "- `slope`, a float,\n",
    "- `x`, a 1D list/array of numbers, and\n",
    "- `y`, a 1D list/array of numbers.\n",
    "\n",
    "`mae_of_candidate_line` should return the mean absolute error from using the line with intercept `intercept` and slope `slope` to predict `y` from `x`.\n",
    "\n",
    "Example behavior is given below.\n",
    "\n",
    "```python\n",
    ">>> mae_of_candidate_line(5, 2, [1, 2, -1, 4], [15, 6, 7, 8])\n",
    "5.0\n",
    "```\n",
    "\n",
    "For more context on the example above:\n",
    "\n",
    "- There are four points in the dataset provided: $(1, 15)$, $(2, 6)$, $(-1, 7)$, and $(4, 8)$.\n",
    "- The line we're using to make predictions is $h(x_i) = 5 + 2x_i$. This line, and the four points above, are visualized below:\n",
    "\n",
    "<center><img src=\"imgs/mae-example.png\" width=400></center>\n",
    "\n",
    "- The absolute errors of the line's predictions are 4, 8, 3, and 5. So, the mean of absolute errors is $\\frac{4+8+3+5}{4} = 5$.\n",
    "\n",
    "Don't use a `for`-loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd39fad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mae_of_candidate_line(intercept, slope, x, y):\n",
    "    ...\n",
    "\n",
    "# Feel free to change this input to make sure your function works correctly.  \n",
    "mae_of_candidate_line(5, 2, [1, 2, -1, 4], [15, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e3b788",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"p05_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc2e09",
   "metadata": {},
   "source": [
    "### Problem 5c) (5 pts)\n",
    "\n",
    "Now, put it all together. Complete the implementation of the `SimpleLAD` class, which has two methods, apart from the constructor.\n",
    "\n",
    "#### `fit`\n",
    "\n",
    "`fit` takes in two* 1D list/arrays, `x` and `y`. Using the previously-defined helper functions, `fit` determines the intercept and slope that minimize mean absolute error on the dataset defined by `x` and `y`.\n",
    "                \n",
    "`fit` should not return anything, but should instead set the values of `self.intercept_` (the optimal intercept) and `self.coef_` (the optimal slope; we use the attribute name `coef_` instead of `slope_` to match `sklearn`'s naming conventions).\n",
    "\n",
    "If there are multiple optimal combinations of intercepts and slopes, set `self.intercept_` and `self.slope_` to any one of those combinations.\n",
    "\n",
    "*As you'll see in the method stub, `fit` takes in a third argument (at the start), named `self`. The role of the `self` argument is to be able to access attributes and methods of the current instance of the `SimpleLAD` class. Read [this article](https://www.geeksforgeeks.org/self-in-python-class/) for more information on the role of the `self` argument in Python.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `predict`\n",
    "\n",
    "`predict` takes in a single (non-`self`) input, named `x_new`, which can either be a single value or list/array of values.\n",
    "\n",
    "- If `x_new` is a single value, `predict` should return a single value, corresponding to the predicted $y$-value for the passed in $x$-value, using the already-found `self.intercept_` and `self.coef_`.\n",
    "- If `x_new` is a list or array, `predict` should return an **array** corresponding to the predict $y$-values for the passed in $x$-values, using the already-found `self.intercept_` and `self.coef_`.\n",
    "\n",
    "`fit` must be called before `predict`; if not, raise an `AttributeError`.\n",
    "\n",
    "<br>\n",
    "\n",
    "Example behavior is given below.\n",
    "\n",
    "```python\n",
    ">>> model = SimpleLAD()\n",
    ">>> model.fit([1, 2, -1, 4], [15, 6, 7, 8])\n",
    ">>> model.intercept_\n",
    "7.2\n",
    ">>> model.coef_\n",
    "0.2\n",
    ">>> model.predict(5)\n",
    "8.2\n",
    ">>> model.predict([5, -3.5, 5])\n",
    "array([8.2, 6.5, 8.2])\n",
    "```\n",
    "\n",
    "For more context on the example above:\n",
    "\n",
    "- There are four points in the dataset provided: $(1, 15)$, $(2, 6)$, $(-1, 7)$, and $(4, 8)$.\n",
    "- The helper functions `generate_pairs`, `generate_lines`, and `mae_of_candidate_line` helped us deduce that the line with the minimum mean absolute error on this dataset is $h(x_i) = 7.2 + 0.2x_i$, so `model.intercept_` is `7.2` and `model.coef_` is `0.2`.\n",
    "- Using the fit hypothesis function $h(x_i) = 7.2 + 0.2x_i$ on the inputs 5, -3.5, and 5 give us the predictions $h(5) = 8.2$, $h(-3.5) = 6.5$, and $h(5) = 8.2$, so we return an array with those three values. (Note that we return an array even though the inputs were provided as a list.) When using this hypothesis function on the single input 5, we return just the value $h(5) = 8.2$, not as an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d7858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleLAD:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        __init__ is the name given to the constructor method in a Python class.\n",
    "        We don't need to do anything to initialize a SimpleLAD object, so this constructor\n",
    "        doesn't actually do anything.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        if len(x) != len(y):\n",
    "            raise ValueError(f'Dimension mismatch: x has length {len(x)} while y has length {len(y)}')\n",
    "            \n",
    "        ...\n",
    "        \n",
    "        # The last two lines in the body of `fit` should be the two below.\n",
    "        self.intercept_ = ...\n",
    "        self.coef_ = ...\n",
    "        \n",
    "    def predict(self, x_new):\n",
    "        if isinstance(x_new, list):\n",
    "            x_new = np.array(x_new)\n",
    "        try:\n",
    "            ...\n",
    "        except AttributeError:\n",
    "            raise AttributeError('Cannot use `predict` before `fit`.')\n",
    "            \n",
    "            \n",
    "# Feel free to change the inputs below to make sure your class implementation works correctly.\n",
    "model = SimpleLAD()\n",
    "model.fit([1, 2, -1, 4], [15, 6, 7, 8])\n",
    "preds = model.predict([5, -3.5, 5])\n",
    "print(f'''\n",
    "model.intercept_ = {model.intercept_}\n",
    "model.coef_ = {model.coef_}\n",
    "model.predict([5, -3.5, 5]) = {preds}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12af708f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"p05_c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae32f709",
   "metadata": {},
   "source": [
    "Now that our implementation of `SimpleLAD` is complete, we can use it to fit real datasets! Run the cell below to load in a table with two columns, `x` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dab82dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_for_lad = pd.read_csv('data/data-for-lad.csv')\n",
    "data_for_lad.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842cf014",
   "metadata": {},
   "source": [
    "Run the cell below to draw a scatter plot of `y` vs. `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d5c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(data_for_lad, x='x', y='y', color_discrete_sequence=['#444']).update_layout(width=800, height=400)\n",
    "fig.show(renderer='png', scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5443c16",
   "metadata": {},
   "source": [
    "There's a clear linear association at the bottom, with some outliers spread throughout the top. Let's see how the best-fitting lines look on this dataset, when the lines are chosen by minimizing mean squared error vs. mean absolute error.\n",
    "\n",
    "First, we'll find the standard simple linear regression line, i.e. the one that minimizes mean squared error. We'll use `sklearn` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00adabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb7ed83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_mse = LinearRegression()\n",
    "model_mse.fit(X=data_for_lad[['x']], y=data_for_lad['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd15678",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An array with one optimal parameter.\n",
    "# sklearn's LinearRegression supports multiple regression, meaning it stores\n",
    "# the coef_ attribute in a way that is flexible enough to hold multiple slope parameters.\n",
    "model_mse.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced66239",
   "metadata": {},
   "source": [
    "Now, let's compute the least absolute deviations line, i.e. the one that minimizes mean absolute error. **This is where your hard work comes in!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9232db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lad = SimpleLAD()\n",
    "model_lad.fit(data_for_lad['x'].to_numpy(), data_for_lad['y'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lad.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1847a321",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lad.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4901c104",
   "metadata": {},
   "source": [
    "Let's graph both of these lines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a401ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = px.scatter(data_for_lad, x='x', y='y', color_discrete_sequence=['#888']).update_layout(width=800, height=400)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[-1, 11],\n",
    "        y=model_mse.predict([[-1], [11]]),\n",
    "        mode='lines',\n",
    "        name='Best Line with Minimizing MSE',\n",
    "        line={'color': '#00274C'}\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[-1, 11],\n",
    "        y=model_lad.predict([-1, 11]),\n",
    "        mode='lines',\n",
    "        name='Best Line when Minimizing MAE',\n",
    "        line={'color': '#FFCB05'}\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show(renderer='png', scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e5c961",
   "metadata": {},
   "source": [
    "What do you notice? There's nothing you need to write or comment on here, but you should think about what makes the lines appear so different, and **why** this is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c1fffb",
   "metadata": {},
   "source": [
    "### Problem 5d) (0 pts, optional)\n",
    "\n",
    "We've built a naïve implementation of simple LAD (least absolute deviations) regression. Suppose $n$ is the number of points in the dataset that we fit a `SimpleLAD` object on. Which of the following most accurately describe the runtime of `SimpleLAD.fit`, in Big-O notation? Assign `naive_lad_runtime` to an integer between 1 and 8, inclusive, corresponding to your answer among the choices below.\n",
    "\n",
    "1. $O(1)$\n",
    "2. $O(n)$\n",
    "3. $O(n^2)$\n",
    "4. $O(n^3)$\n",
    "5. $O(\\log n)$\n",
    "6. $O(n \\log n)$\n",
    "7. $O(n!)$\n",
    "8. $O(2^n)$\n",
    "\n",
    "There are no hidden tests here, and this part is worth 0 points.\n",
    "\n",
    "_Hint: When computing the theoretical runtime of an algorithm, it doesn't matter which language or package an operation is implemented in – a fast `numpy` vectorized operation still involves a loop!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be170fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "naive_lad_runtime = ...\n",
    "naive_lad_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e773c15",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"p05_d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9690166e",
   "metadata": {},
   "source": [
    "## Finish Line 🏁\n",
    "\n",
    "Congratulations! You're ready to submit the programming portion of Homework 2.\n",
    "\n",
    "To submit your work to Gradescope:\n",
    "\n",
    "1. Select `Kernel -> Restart & Run All` to ensure that you have executed all cells, including the test cells.\n",
    "2. Read through the notebook to make sure everything is fine and all public tests passed.\n",
    "3. Run the cell below to run all tests, and make sure that they all pass.\n",
    "4. Download your notebook using `File -> Download`, then upload your notebook to Gradescope under \"Homework 2, Problem 5 Code\".\n",
    "5. Stick around for a few minutes while the Gradescope autograder grades your work. Make sure you see that all **public tests** have passed on Gradescope. **Remember that homeworks have hidden tests!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "otter": {
   "tests": {
    "p05_a": {
     "name": "p05_a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = generate_lines([((1, 2), (3, 7)), ((1, 10), (-4, 20))])\n>>> desired_out = [(-0.5, 2.5), (12.0, -2.0)]\n>>> is_close_pair = lambda a, b: np.isclose(a[0], b[0]) and np.isclose(a[1], b[1])\n>>> all([is_close_pair(out[i], desired_out[i]) for i in range(len(out))])\nTrue",
         "failure_message": "generate_lines([((1, 2), (3, 7)), ((1, 10), (-4, 20))]) returns the wrong values.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> out = generate_lines([((0, 0), (1, 1))])\n>>> desired_out = [(0.0, 1.0)]\n>>> is_close_pair = lambda a, b: np.isclose(a[0], b[0]) and np.isclose(a[1], b[1])\n>>> all([is_close_pair(out[i], desired_out[i]) for i in range(len(out))])\nTrue",
         "failure_message": "generate_lines([((0, 0), (1, 1))]) returns the wrong values.",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "p05_b": {
     "name": "p05_b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = mae_of_candidate_line(5, 2, [1, 2, -1, 4], [15, 6, 7, 8])\n>>> np.isclose(out, 5)\nTrue",
         "failure_message": "mae_of_candidate_line(5, 2, [1, 2, -1, 4], [15, 6, 7, 8]) returns the wrong value.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> out = mae_of_candidate_line(0.5, 1, list(np.arange(10)), list(np.arange(1, 11)))\n>>> np.isclose(out, 0.5)\nTrue",
         "failure_message": "mae_of_candidate_line(0.5, 1, np.arange(10), np.arange(1, 11)) returns the wrong value.",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "p05_c": {
     "name": "p05_c",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> model_test = SimpleLAD()\n>>> model_test.fit([1, 2, -1, 4], [15, 6, 7, 8])\n>>> np.isclose(model_test.intercept_, 7.2) and np.isclose(model_test.coef_, 0.2)\nTrue",
         "failure_message": "When fit on x=[1, 2, -1, 4], y=[15, 6, 7, 8], model object computes incorrect optimal parameters.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> model_test = SimpleLAD()\n>>> model_test.fit([1, 2, -1, 4], [15, 6, 7, 8])\n>>> np.allclose(model_test.predict([5, -3.5, 5]), [8.2, 6.5, 8.2])\nTrue",
         "failure_message": "When fit on x=[1, 2, -1, 4], y=[15, 6, 7, 8], model object computes incorrect predictions.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> model_test = SimpleLAD()\n>>> model_test.fit(np.array([1.5, -2.9, 3, 14, -3, 2, np.pi]), [0.8, -0.71, 1776, np.e ** 2, 14, 3, 9])\n>>> np.isclose(model_test.intercept_, 2.268490650178225) and np.isclose(model_test.coef_, 0.3657546749108875)\nTrue",
         "failure_message": "When fit on x=np.array([1.5, -2.9, 3, 14, -3, 2, np.pi]), y=[0.8, -0.71, 1776, np.e ** 2, 14, 3, 9], model object computes incorrect optimal parameters.",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "p05_d": {
     "name": "p05_d",
     "points": 0,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> naive_lad_runtime in range(1, 9)\nTrue",
         "failure_message": "Answer must be an integer between 1 and 8, inclusive.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> naive_lad_runtime == 4\nTrue",
         "failure_message": "There are (n choose 2) = (n^2 - n / 2) = O(n^2) combinations of points, and for each combination we compute the MAE, which requires a pass of the entire dataset. So, we need n^2 * n = O(n^3) operations to compute the optimal line.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> naive_lad_runtime in {3, 4}\nTrue",
         "failure_message": "Partial credit for answering O(n^2).",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
