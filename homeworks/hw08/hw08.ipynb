{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a59cacd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "#### Homework 8 Supplemental Notebook\n",
    "    \n",
    "# Multiple Linear Regression\n",
    "\n",
    "### EECS 245, Fall 2025 at the University of Michigan\n",
    "    \n",
    "</div>\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Most homeworks will have Jupyter Notebooks, like this one, designed to supplement the theoretical problems. \n",
    "\n",
    "To write and run code in this notebook, you have two options:\n",
    "\n",
    "1. **Use the EECS 245 DataHub.** To do this, click the link provided in the Homework 8 PDF. Before doing so, read the instructions on the [**Tech Support**](https://eecs245.org/tech-support/#option-1-using-the-eecs-245-datahub) page on how to use the DataHub.\n",
    "1. **Set up a Jupyter Notebook environment locally, and use `git` to clone our course repository.** For instructions on how to do this, see the [**Tech Support**](https://eecs245.org/tech-support) page of the course website.\n",
    "\n",
    "**You do not need to submit this notebook to Gradescope.** Instead, you'll be told to screenshot your implementations of various tasks and include them in your Homework 8 PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set default layout for all plotly figures\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "custom_template = go.layout.Template(pio.templates[\"plotly_white\"])\n",
    "custom_template.layout.plot_bgcolor = \"white\"\n",
    "custom_template.layout.paper_bgcolor = \"white\"\n",
    "custom_template.layout.margin = dict(l=60, r=60, t=60, b=60)\n",
    "custom_template.layout.width = 700\n",
    "custom_template.layout.font = dict(\n",
    "    family=\"Palatino Linotype, Palatino, serif\",\n",
    "    color=\"black\"\n",
    ")\n",
    "\n",
    "pio.templates[\"custom\"] = custom_template\n",
    "pio.templates.default = \"custom\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0858cab",
   "metadata": {},
   "source": [
    "## Problem 4: The Complete Solution\n",
    "\n",
    "---\n",
    "\n",
    "We're intentionally not providing any starter code for this problem. Instead, peek through [Chapter 3.1](https://notes.eecs245.org/multiple-linear-regression/regression-using-linear-algebra/) and [Chapter 3.2](https://notes.eecs245.org/multiple-linear-regression/multiple-linear-regression/) of the course notes, or later in this notebook, to see how to write the relevant code. It should only take a few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd6145e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fafbeda",
   "metadata": {},
   "source": [
    "## Problem 5: Home Run!\n",
    "\n",
    "---\n",
    "\n",
    "Below, we load in a dataset that describes the [number of home runs in the MLB per year](https://www.mlb.com/glossary/standard-stats/home-run). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a9ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "homeruns = pd.read_csv('data/homeruns.csv')\n",
    "homeruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ef0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = homeruns.plot(kind='scatter', x='Year', y='Homeruns', title='Homeruns vs. Year')\n",
    "fig.show(renderer='notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88919537",
   "metadata": {},
   "source": [
    "Remember, this problem is **not autograded**. Instead, in each part, include screenshots of your code and a scatter plot of your model's predictions, and write the formula for your fit model, in your Homework 8 PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ed9f25",
   "metadata": {},
   "source": [
    "### Problem 5a)\n",
    "\n",
    "$$h(x_i) = w_0 + w_1 x_i + w_2 x_i^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91efcd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression # Only needs to be imported once.\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def plot_raw_data_and_predictions(X_raw, X_proc, y, model):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - X_raw, a DataFrame with a single column, `Year`\n",
    "        - X_proc, a DataFrame that results from transforming X_raw with a feature creator\n",
    "        - y, a Series with `Homeruns` values\n",
    "        - model, an **already fit** `LinearRegression` objects\n",
    "    Outputs:\n",
    "        - Returns a plotly figure object showing a scatter plot of the raw data in blue, with the model's predictions overlaid in orange\n",
    "\n",
    "    \"\"\"\n",
    "    # Reuse me!\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_raw.to_numpy().flatten(),\n",
    "            y=y,\n",
    "            mode='markers',\n",
    "            name='Actual'\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_raw.to_numpy().flatten(),\n",
    "            y=model.predict(X_proc),\n",
    "            mode='lines',\n",
    "            name='Predictions',\n",
    "            line=dict(color='orange', width=4)\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title='Homeruns vs. Year (with Model Predictions)',\n",
    "        xaxis_title='Year',\n",
    "        yaxis_title='Homeruns'\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def process_and_fit_model(X, y, feature_creator):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - X, a DataFrame with a single column, `Year`\n",
    "        - y, a Series with `Homeruns` values\n",
    "        - feature_creator, a function that takes in X and returns a DataFrame with the new added features\n",
    "    Outputs:\n",
    "        - Fits a `LinearRegression` model on the transformed data, prints the optimal parameters, and prints the mean squared error of the model's predictions\n",
    "        - Draws a plot of the raw data in blue, with the model's predictions overlaid in orange\n",
    "    \"\"\"\n",
    "\n",
    "    # Create features\n",
    "    X_raw = X.copy()\n",
    "    X_proc = feature_creator(X)\n",
    "\n",
    "    # Instantiate and fit model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X=X_proc, y=y)\n",
    "\n",
    "    # Compute predictions and mean squared error\n",
    "    preds = model.predict(X=X_proc)\n",
    "    rmse = mean_squared_error(y, preds) ** 0.5\n",
    "    print('optimal parameters:', model.intercept_, model.coef_)\n",
    "    print('root mean squared error:', rmse)\n",
    "    print('r^2', model.score(X_proc, y))\n",
    "\n",
    "    # Draw plot of predictions\n",
    "    fig = plot_raw_data_and_predictions(X_raw, X_proc, y, model)\n",
    "    fig.show(renderer='notebook')\n",
    "\n",
    "def create_feature_columns_5a(X):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - X, a DataFrame with a single column, `Year`\n",
    "    Outputs:\n",
    "        - Returns a DataFrame with two columns: `Year` and `Year^2`\n",
    "    \"\"\"       \n",
    "    X = X.copy() # Don't forget this; otherwise, you may make in-place modifications to the raw data.\n",
    "    X['Year'] = X['Year'] # Already there; just including this line to show that it hasn't been removed.\n",
    "    X['Year^2'] = X['Year'] ** 2\n",
    "    return X\n",
    "\n",
    "process_and_fit_model(homeruns[['Year']], homeruns['Homeruns'], create_feature_columns_5a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdacb141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1617fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6ad15e",
   "metadata": {},
   "source": [
    "## Problem 6: Polynomial Regression\n",
    "\n",
    "---\n",
    "\n",
    "Run the cell below to generate the dataset for this problem. **Do not** modify the code in this cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11325c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23) # For reproducibility.\n",
    "\n",
    "def sample_from_pop(n):\n",
    "    x = np.linspace(-3, 3, n)\n",
    "    y = (x**3) + (np.random.normal(0, 5, size=n))\n",
    "    return pd.DataFrame({'x': x, 'y': y})\n",
    "\n",
    "full = sample_from_pop(n=200)\n",
    "full.plot(kind='scatter', x='x', y='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e3979",
   "metadata": {},
   "source": [
    "Here, we'll fit 25 polynomial models (one with degree 1, one with degree 2, ..., one with degree 25). But, as discussed in the PDF, we won't train each model on the entire dataset; instead, we'll split the data into training and test sets, train on the training set, and then evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcdf5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets.\n",
    "# The random_state ensures that we get the same \"split\" of the data each time we run this cell.\n",
    "X_train, X_test, y_train, y_test = train_test_split(full[['x']], full['y'], test_size=0.2, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790b861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed188882",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0274dd0",
   "metadata": {},
   "source": [
    "Instead of creating the features manually, like in Problem 5, we'll use some more advanced features in `sklearn` to create the features for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9091514a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d82ec7",
   "metadata": {},
   "source": [
    "Below, we create a **Pipeline** that first creates polynomial features of degree **20** (chosen as an example), and then fits a `LinearRegression` model on the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfe63a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(degree=20, include_bias=False), LinearRegression())\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f17f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1731ccd0",
   "metadata": {},
   "source": [
    "The model's training set and test set mean squared errors are below. (Unlike in Problem 5, we're using MSE instead of RMSE since the $y$-axis scale is already relatively small.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e894c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse = mean_squared_error(y_train, model.predict(X_train))\n",
    "test_mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "print(f\"Training MSE: {train_mse:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1441462",
   "metadata": {},
   "source": [
    "Note that the model performed significantly worse on the test set. Let's look at the model overlaid on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1207bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_model_train_test(model, X_train, y_train, X_test, y_test):\n",
    "    # Make prediction domain, but clip xs to [-2, 2.5]\n",
    "    xs = np.linspace(-3, 3, 300).reshape(-1, 1)\n",
    "    y_pred = model.predict(xs)\n",
    "    \n",
    "    # Set up figure with 1 row, 2 columns\n",
    "    train_mse = mean_squared_error(y_train, model.predict(X_train))\n",
    "    test_mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "    degree = model[0].degree\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=(f\"Training Data (MSE for degree {degree}: {train_mse:.2f})\", f\"Test Data (MSE for degree {degree}: {test_mse:.2f})\"))\n",
    "\n",
    "    # (1) Left: training data\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=X_train.squeeze(), y=y_train, mode='markers', name=\"Train data\"),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    # Model prediction curve (orange)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=xs.squeeze(), y=y_pred, mode='lines', name=\"Predictions\", line=dict(color='orange', width=4)),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # (2) Right: test data\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=X_test.squeeze(), y=y_test, mode='markers', name=\"Test data\"),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    # Model prediction curve (orange)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=xs.squeeze(), y=y_pred, mode='lines', name=\"Predictions\", showlegend=False, line=dict(color='orange', width=4)),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Clip x-axes to [-2, 2.5] in both subplots\n",
    "    fig.update_xaxes(title_text=\"X\", range=[-3, 3], row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"y\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"X\", range=[-3, 3], row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"y\", row=1, col=2)\n",
    "\n",
    "    fig.update_layout(height=400, width=900, showlegend=True)\n",
    "    fig.show(renderer='notebook')\n",
    "\n",
    "plot_model_train_test(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ea1e26",
   "metadata": {},
   "source": [
    "Note that the model performed significantly worse on the test set. This, at some level, is expected; the model got to see the training data when choosing $\\vec w^*$, and so it may have fit the training data too closely. But is 20 the best degree?\n",
    "\n",
    "**Your Job**: Fill in the blanks below to complete the tasks mentioned in the PDF.\n",
    "\n",
    "1. Fit 25 polynomial regression models on the training set (degree 1, degree 2, ..., degree 25).\n",
    "1. For each model compute both its training and test mean squared error.\n",
    "1. Create a line plot of the training and test mean squared error vs. degree. (There is helper code for this below.)\n",
    "1. **Include screenshots of all of your code and the resulting plot in your Homework 8 PDF.**\n",
    "\n",
    "That is, include screenshots of everything between the <span style=\"color:orange; font-weight:bold;\">orange lines</span>.\n",
    "\n",
    "<hr style=\"border: 0; height: 4px; background: orange;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a194fda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_two_lines(deg, train_mses, test_mses):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=deg, y=train_mses, mode='lines+markers', name='Train MSE'))\n",
    "    fig.add_trace(go.Scatter(x=deg, y=test_mses, mode='lines+markers', name='Test MSE'))\n",
    "    fig.update_layout(\n",
    "        title=\"Training and Test MSE vs. Degree\",\n",
    "        xaxis_title='Degree',\n",
    "        yaxis_title='MSE'\n",
    "    )\n",
    "    fig.show(renderer='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b56c30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f104af87",
   "metadata": {},
   "source": [
    "<hr style=\"border: 0; height: 4px; background: orange;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40136fc",
   "metadata": {},
   "source": [
    "## Problem 7: One Hot Encoding in `sklearn`\n",
    "\n",
    "---\n",
    "\n",
    "In this problem, we'll aim to build a regression model that predicts the price of a house based on various features about it. The dataset we're using, originally compiled by Professor Dean De Cock at Truman State University **specifically for** teaching regression, contains information about houses sold in Ames, Iowa from 2006 to 2010.\n",
    "\n",
    "Run the cell below to load in the data. A full data dictionary can be found [here](https://www.openintro.org/data/index.php?data=ames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f4e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "houses = pd.read_csv('data/iowa.csv')\n",
    "houses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fc3167",
   "metadata": {},
   "source": [
    "Each row corresponds to a house. There are 80 columns (so 79 possible features); the last column contains the target variable, `SalePrice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "houses.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96554a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = houses.plot(kind='scatter', x='Gr Liv Area', y='SalePrice')\n",
    "fig.update_layout(title='Square Footage (Excluding Basement) vs. Sale Price')\n",
    "fig.show(renderer='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5292e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = houses['Neighborhood'].value_counts().plot(kind='bar')\n",
    "fig.update_layout(title='Distribution of Neighborhoods')\n",
    "fig.show(renderer='notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02890239",
   "metadata": {},
   "source": [
    "As we learned in Problem 6, we should always perform a train/test split before building a model. Let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c7d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(houses.drop(columns=['SalePrice']), houses['SalePrice'], test_size=0.2, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bd34b1",
   "metadata": {},
   "source": [
    "In Problem 6, we looked at how to create an `sklearn` Pipeline that first creates polynomial features using the `PolynomialFeatures` class, and then fits a `LinearRegression` model on the transformed data. But there, we wanted to create polynomial features out of every single \"origin\" feature, which was just the $x$-variable. But here, it might make sense to use `Gr Liv Area` (non-basement square footage) as a numerical feature as-is, and then one hot encode categorical features, like `Neighborhood`.\n",
    "\n",
    "The big idea is that we need a way to tell `sklearn` what operations to apply on each feature. The solution is to use a `ColumnTransformer` object, which allows us to specify different operations for different columns. Let's look at an example of how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb4ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "\n",
    "model = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        (OneHotEncoder(drop='first', handle_unknown='ignore'), ['Neighborhood']),\n",
    "        (FunctionTransformer(lambda x: x, validate=False), ['Gr Liv Area']),\n",
    "        remainder='drop'\n",
    "    ),\n",
    "    LinearRegression()\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a88c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5049ad",
   "metadata": {},
   "source": [
    "You might have notice that after fitting the model, it appears blue (the same happened in the previous problem, too).\n",
    "\n",
    "A lot of syntax went into defining `model`. Let's break it down:\n",
    "1. First, `model` itself is a Pipeline. Its first step is a `ColumnTransformer`, which is the \"controller\" that tells `sklearn` what to do with each column. The second step is a `LinearRegression` model.\n",
    "1. The `ColumnTransformer`, created using `make_column_transformer`, is instantiated using several tuples, each one containing the name of a transformation and a list of relevant columns.\n",
    "    - We said apply `OneHotEncoder(drop='first', handle_unknown='ignore')` to the `Neighborhood` column.\n",
    "    - `FunctionTransformer(lambda x: x, validate=False)` looks a little strange, but all it's saying is to **keep `Gr Liv Area` as is**.\n",
    "    - `remainder='drop'` tells `sklearn` to drop the columns that are not mentioned in the list of tuples.\n",
    "1. The `LinearRegression` model is the usual `LinearRegression` model.\n",
    "\n",
    "We can peek into how the model was fit and transformed our data. First, we can look at its coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f001a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access step 2 of the Pipeline, which is the LinearRegression model.\n",
    "model[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf43d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[1].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8da399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of parameters in the model (including the intercept term, which is not accounted for above.)\n",
    "len(model[1].coef_) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4475e03c",
   "metadata": {},
   "source": [
    "Many of the coefficients above correspond to one hot encoded features. Let's look at the names of the one hot encoded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c75e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[0]['onehotencoder'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8afa8d",
   "metadata": {},
   "source": [
    "How many one hot encoded features did the model create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77b45a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model[0]['onehotencoder'].get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5618c4",
   "metadata": {},
   "source": [
    "But, in `X_train`, how many unique `Neighborhood` values are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5723be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Neighborhood'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc8ae42",
   "metadata": {},
   "source": [
    "Hmmm... what gives? The reason that our model produced one fewer one hot encoded features than there were unique `Neighborhood` values is that we told `sklearn` to drop one category of `Neighborhood` (the first one that it saw, to be exact). We did this to ensure that the resulting model's design matrix was of full column rank, meaning that all of its columns are linearly independent. As we've discussed in [Chapter 2.10](https://notes.eecs245.org/vectors-and-matrices/projection-2/) and, more recently, [Chapter 3.2](https://notes.eecs245.org/multiple-linear-regression/multiple-linear-regression/), this ensures that we have a unique solution for $\\vec w^*$, our optimal parameter vector.\n",
    "\n",
    "The cool thing is that to use our new model to make predictions, we can just use the usual `.predict` method, and don't need to worry about re-implementing the logic of one hot encoding.\n",
    "\n",
    "For example, let's predict how much a house with 2550 square feet of living area in the `'CollgCr'` neighborhood is estimated to sell for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13d7429",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(pd.DataFrame({'Gr Liv Area': [2550], 'Neighborhood': ['CollgCr']}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39808acd",
   "metadata": {},
   "source": [
    "Just under \\$285,000, it seems!\n",
    "\n",
    "\n",
    "What if we make up a new neighborhood, like `'Ann Arbor'`, which definitely doesn't exist in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff0dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(pd.DataFrame({'Gr Liv Area': [2550], 'Neighborhood': ['Ann Arbor']}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3adfaec",
   "metadata": {},
   "source": [
    "We get a `UserWarning`, but not an error, because we instantiated `OneHotEncoder` with `handle_unknown='ignore'`, which tells it to ignore any `Neighborhood`categories it sees in `.predict` that it didn't see in `.fit`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b070c7e0",
   "metadata": {},
   "source": [
    "What is this model's training and test mean squared error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fdda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse = mean_squared_error(y_train, model.predict(X_train))\n",
    "test_mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "print(f\"Training MSE: {train_mse:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0430c78",
   "metadata": {},
   "source": [
    "\n",
    "**Your Job**: Below, build a Pipeline that:\n",
    "\n",
    "- Creates `PolynomialFeatures` of degree 3 for the `Gr Liv Area` column (make sure to set `include_bias=False`, since `LinearRegression` will add its own intercept term).\n",
    "- Uses the `Yr Sold` and `Year Built` columns as-is.\n",
    "- One hot encodes **both** the `Neighborhood` column and the `MS SubClass` columns.\n",
    "- Trains a `LinearRegression` model on the transformed data, ignoring/dropping any other columns.\n",
    "\n",
    "In your notebook, include:\n",
    "\n",
    "1. Screenshots of your code and the model's training and test mean squared errors.\n",
    "2. The number of parameters in the model, including the intercept term; include screenshots of the code you used to find this number.\n",
    "3. A 1-2 sentence explanation of why the model has this number of parameters.\n",
    "\n",
    "<hr style=\"border: 0; height: 4px; background: orange;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d60151e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c134a6a1",
   "metadata": {},
   "source": [
    "<hr style=\"border: 0; height: 4px; background: orange;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4855a8",
   "metadata": {},
   "source": [
    "## Finish Line 🏁\n",
    "\n",
    "Remember, this notebook does not need to be submitted to Gradescope! Make sure you've included the necessary screenshots for Problems 4-7 in your Homework 8 PDF."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
